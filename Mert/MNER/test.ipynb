{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('data/twitter2015/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='exercise\tO\\n'.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding=tokenizer('I love w/ !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.index('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoding.tokens())\n",
    "print(encoding.word_ids())\n",
    "print(encoding.token_to_chars(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag={0:'O',1:'B-PER',2:'I-PER',3:'B-LOC',4:'I-LOC',5:'B-ORG',6:'I-ORG',7:'B-OTHER',8:'I-OTHER'}\n",
    "tag2id={'O':0,'B-PER':1,'I-PER':2,'B-LOC':3,'I-LOC':4,'B-ORG':5,'I-ORG':6,'B-OTHER':7,'I-OTHER':8}\n",
    "ESD_id2tag={0:'O',1:'B',2:'I'}\n",
    "ESD_tag2id={'O':0,'B':1,'I':2}\n",
    "Data={}\n",
    "idx=0\n",
    "ind=0\n",
    "end=len(dataset)\n",
    "while ind < end:\n",
    "    text=dataset[ind]\n",
    "    if text[:5]=='IMGID':\n",
    "        img_id=text[6:-1]\n",
    "        ind+=1\n",
    "        if dataset[ind][:2]=='RT':# skip name\n",
    "            ind+=3\n",
    "        #read sentence\n",
    "        sentence=''\n",
    "        tags=[]\n",
    "        ESD_tags=[]\n",
    "        while ind < end:\n",
    "            text = dataset[ind]\n",
    "            if text=='\\n':#reach the end of a sample\n",
    "                ind+=1\n",
    "                break\n",
    "            if text[:4]=='http':#skip url\n",
    "                ind+=1\n",
    "                continue\n",
    "            word,tag=text.replace('\\n','').split('\\t')\n",
    "            if sentence!='':#use space to split words\n",
    "                sentence+=' '\n",
    "                tags.append(0)\n",
    "                ESD_tags.append(0)\n",
    "            sentence+=word\n",
    "            #tag is mapped to char\n",
    "            tags+=[tag2id[tag]]*len(word)\n",
    "            ESD_tags+=[ESD_tag2id[tag[0]]]*len(word)\n",
    "            ind+=1\n",
    "        Data[idx]={'imgid':img_id,'sentence':sentence,'tags':tags,'ESD_tags':ESD_tags}\n",
    "        idx+=1\n",
    "    else:\n",
    "        ind+=1\n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=b.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TwitterDataset\n",
    "from utils import TwitterColloteFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "train_data=TwitterDataset(config.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_data,batch_size=4,shuffle=True,collate_fn=TwitterColloteFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs,img_inputs,tags,ESD_tags=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.W_e2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import FlavaProcessor, FlavaModel\n",
    "\n",
    "model = FlavaModel.from_pretrained(\"facebook/flava-full\")\n",
    "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open('/home/zero_lag/Document/srtp/data/twitter2015/images/27.jpg')\n",
    "\n",
    "inputs = processor(\n",
    "  text=[\"a photo of a cat\", \"a photo of a dog\"], images=[image, image], return_tensors=\"pt\", padding=\"max_length\", max_length=77\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=processor(\n",
    "  text=[\"a photo of a cat\", \"a photo of a dog and cat and cat\"], images=[image, image], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "image_embeddings = outputs.image_embeddings # Batch size X (Number of image patches + 1) x Hidden size => 2 X 197 X 768\n",
    "text_embeddings = outputs.text_embeddings # Batch size X (Text sequence length + 1) X Hidden size => 2 X 77 X 768\n",
    "multimodal_embeddings = outputs.multimodal_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(a,b):\n",
    "    print(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'a':1,'b':2,'c':3}\n",
    "a(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del args['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\", max_length=77)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=open(config.train_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.readlines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TwitterDataset,TwitterColloteFn\n",
    "from config import config\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TwitterDataset(file_path='/home/zero_lag/Document/srtp/Multimodality-Link/Mert/MNER/data/Twitter2015/train.txt',img_path=config.train_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(train_data,batch_size=2,shuffle=True,collate_fn=TwitterColloteFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(input,label,ESD_label) in enumerate(train_dataloader):\n",
    "    mask=input['attention_mask']\n",
    "    if not mask[:0].all():\n",
    "        print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=Image.open('/home/zero_lag/Document/srtp/data/twitter2015/images/2069958.jpg')\n",
    "img2=Image.open('/home/zero_lag/Document/srtp/data/twitter2015/images/1060749.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=tensor([1,0],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.bool().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-cased')\n",
    "encoder=BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer('I love you',return_tensors=\"pt\", padding=\"max_length\", max_length=config.max_length,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=encoder(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir('/home/zero_lag/Document/srtp/data/twitter2015/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Image.open('/home/zero_lag/Document/srtp/data/twitter2015/images/73813.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=b[:,:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,g,b=a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=Image.merge('RGB',a.split()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=w.repeat(4,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=train_data.W_e2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=w.repeat(4,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Image.open('/home/zero_lag/Document/srtp/data/twitter2015/images/109338.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3]\n",
    "b=[4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(a,b):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CRF(3,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission=torch.randn(2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=torch.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels=model.decode(emission,mask.bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(10) if i%2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1,2,3]]+[[4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join('/home','test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'abc'.split('b')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[14,1,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'mmm_image_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'mmm_text_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'mmm_image_head.decoder.bias', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'mim_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'mmm_image_head.bias', 'mim_head.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'mmm_text_head.transform.dense.weight', 'mlm_head.transform.LayerNorm.weight', 'itm_head.pooler.dense.bias', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'mlm_head.transform.LayerNorm.bias', 'mim_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'mmm_text_head.transform.dense.bias', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'mmm_text_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.input.bias', 'mlm_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'mlm_head.bias', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'mim_head.decoder.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'itm_head.seq_relationship.bias', 'itm_head.seq_relationship.weight', 'mlm_head.transform.dense.weight', 'mmm_text_head.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight']\n",
      "- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaTextModel: ['flava.image_model.encoder.layer.1.attention.attention.value.bias', 'flava.image_model.encoder.layer.6.attention.attention.query.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.3.attention.attention.key.bias', 'mmm_image_head.transform.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.bias', 'flava.image_projection.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.7.attention.output.dense.weight', 'mmm_text_head.transform.LayerNorm.weight', 'mmm_image_head.transform.LayerNorm.bias', 'flava.image_model.encoder.layer.6.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.weight', 'flava.image_model.encoder.layer.10.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'flava.text_projection.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.bias', 'flava.image_model.encoder.layer.6.attention.attention.key.weight', 'flava.text_to_mm_projection.weight', 'flava.image_model.encoder.layer.5.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.3.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.10.layernorm_after.bias', 'flava.image_model.encoder.layer.9.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.5.output.dense.bias', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.weight', 'flava.image_projection.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'flava.multimodal_model.encoder.layer.3.layernorm_after.weight', 'flava.image_model.encoder.layer.1.attention.attention.query.weight', 'flava.image_model.encoder.layer.10.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.0.layernorm_before.bias', 'mim_head.transform.dense.weight', 'flava.image_model.encoder.layer.1.attention.attention.key.weight', 'mlm_head.transform.LayerNorm.weight', 'flava.image_model.encoder.layer.1.attention.attention.key.bias', 'flava.image_model.encoder.layer.8.intermediate.dense.bias', 'flava.image_model.encoder.layer.2.attention.output.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.2.layernorm_after.weight', 'flava.multimodal_model.layernorm.bias', 'flava.multimodal_model.encoder.layer.5.output.dense.weight', 'image_codebook.blocks.output.conv.bias', 'flava.image_model.encoder.layer.5.layernorm_after.weight', 'flava.image_model.encoder.layer.4.attention.attention.value.bias', 'flava.image_model.encoder.layer.10.output.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.1.attention.output.dense.bias', 'flava.image_model.encoder.layer.2.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.1.attention.output.dense.weight', 'flava.image_model.encoder.layer.7.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'mmm_text_head.decoder.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.weight', 'flava.image_model.encoder.layer.2.intermediate.dense.bias', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.weight', 'flava.image_model.encoder.layer.4.output.dense.weight', 'flava.image_model.encoder.layer.0.attention.attention.key.bias', 'image_codebook.blocks.output.conv.weight', 'flava.image_model.encoder.layer.10.attention.attention.key.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.weight', 'flava.image_model.encoder.layer.5.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.weight', 'flava.image_model.encoder.layer.9.attention.attention.query.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_before.weight', 'flava.image_model.encoder.layer.2.attention.attention.query.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.7.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.bias', 'flava.image_model.encoder.layer.11.layernorm_after.bias', 'flava.image_model.encoder.layer.11.output.dense.bias', 'flava.image_model.embeddings.patch_embeddings.projection.bias', 'flava.image_model.encoder.layer.7.layernorm_before.bias', 'flava.image_model.encoder.layer.1.layernorm_after.weight', 'flava.image_model.encoder.layer.0.layernorm_before.weight', 'flava.image_model.encoder.layer.6.attention.attention.value.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.embeddings.position_embeddings', 'flava.multimodal_model.encoder.layer.0.output.dense.weight', 'flava.image_model.encoder.layer.3.attention.attention.query.weight', 'mlm_head.transform.dense.weight', 'flava.image_model.encoder.layer.7.output.dense.bias', 'flava.image_model.encoder.layer.8.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_after.weight', 'flava.image_model.encoder.layer.8.attention.attention.query.weight', 'flava.image_model.layernorm.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.3.layernorm_after.bias', 'flava.image_model.encoder.layer.7.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.output.dense.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.2.layernorm_after.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.3.attention.output.dense.weight', 'flava.image_model.encoder.layer.0.intermediate.dense.weight', 'flava.image_model.encoder.layer.0.attention.attention.value.weight', 'flava.image_model.encoder.layer.9.layernorm_before.bias', 'flava.image_model.encoder.layer.6.intermediate.dense.bias', 'flava.image_model.encoder.layer.6.layernorm_after.weight', 'flava.logit_scale', 'flava.image_model.encoder.layer.2.attention.attention.query.bias', 'flava.image_model.encoder.layer.9.attention.attention.key.weight', 'flava.image_model.encoder.layer.0.intermediate.dense.bias', 'flava.image_model.encoder.layer.3.attention.attention.key.weight', 'flava.image_model.encoder.layer.0.layernorm_before.bias', 'flava.image_model.encoder.layer.8.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.weight', 'flava.image_model.encoder.layer.9.attention.attention.value.bias', 'flava.image_model.encoder.layer.8.layernorm_after.weight', 'flava.image_model.encoder.layer.6.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.bias', 'flava.image_model.encoder.layer.6.layernorm_before.bias', 'mmm_image_head.decoder.bias', 'flava.image_model.encoder.layer.5.attention.attention.key.weight', 'flava.image_model.encoder.layer.10.layernorm_before.bias', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'flava.multimodal_model.pooler.dense.weight', 'mim_head.bias', 'mim_head.transform.LayerNorm.bias', 'flava.image_model.encoder.layer.2.attention.attention.key.weight', 'flava.image_model.encoder.layer.0.layernorm_after.bias', 'flava.image_model.encoder.layer.0.attention.attention.value.bias', 'flava.image_model.encoder.layer.0.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.weight', 'flava.image_model.encoder.layer.8.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.weight', 'flava.image_model.encoder.layer.0.attention.attention.key.weight', 'flava.image_model.encoder.layer.4.intermediate.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.10.output.dense.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'flava.image_model.pooler.dense.weight', 'flava.image_model.embeddings.patch_embeddings.projection.weight', 'flava.image_model.encoder.layer.1.layernorm_after.bias', 'flava.image_model.encoder.layer.1.attention.attention.value.weight', 'mmm_text_head.transform.LayerNorm.bias', 'flava.image_model.encoder.layer.10.layernorm_after.weight', 'flava.image_model.encoder.layer.7.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.3.output.dense.bias', 'flava.image_model.encoder.layer.6.output.dense.bias', 'flava.image_model.encoder.layer.7.attention.attention.value.bias', 'flava.image_model.encoder.layer.8.layernorm_before.bias', 'flava.image_model.encoder.layer.11.layernorm_before.weight', 'flava.image_model.encoder.layer.6.layernorm_after.bias', 'flava.image_model.encoder.layer.0.output.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.bias', 'flava.image_model.embeddings.cls_token', 'flava.multimodal_model.pooler.dense.bias', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.weight', 'flava.image_model.encoder.layer.4.layernorm_after.bias', 'flava.image_model.encoder.layer.2.output.dense.bias', 'flava.image_model.encoder.layer.9.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'flava.multimodal_model.encoder.layer.2.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.bias', 'flava.image_model.encoder.layer.9.intermediate.dense.bias', 'flava.image_model.encoder.layer.0.layernorm_after.weight', 'flava.image_model.encoder.layer.1.intermediate.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.weight', 'flava.image_model.encoder.layer.4.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.3.layernorm_before.bias', 'flava.image_model.encoder.layer.4.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.bias', 'flava.image_model.encoder.layer.5.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.bias', 'itm_head.seq_relationship.weight', 'flava.image_model.encoder.layer.3.intermediate.dense.bias', 'mmm_text_head.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'flava.image_model.encoder.layer.4.layernorm_before.weight', 'flava.image_model.encoder.layer.5.intermediate.dense.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'mmm_text_head.decoder.bias', 'flava.image_model.encoder.layer.8.intermediate.dense.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.8.attention.output.dense.bias', 'flava.image_model.encoder.layer.11.intermediate.dense.weight', 'flava.image_model.encoder.layer.11.intermediate.dense.bias', 'flava.image_model.encoder.layer.4.attention.output.dense.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.9.attention.attention.query.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.11.output.dense.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'flava.image_model.encoder.layer.9.output.dense.bias', 'flava.image_model.encoder.layer.3.attention.output.dense.bias', 'flava.image_model.encoder.layer.11.attention.attention.value.bias', 'flava.text_projection.bias', 'flava.image_model.encoder.layer.2.attention.output.dense.bias', 'flava.image_model.encoder.layer.6.attention.output.dense.weight', 'flava.image_model.encoder.layer.10.attention.output.dense.bias', 'flava.image_model.encoder.layer.9.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.2.output.dense.bias', 'flava.image_model.encoder.layer.10.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_after.weight', 'flava.image_model.embeddings.mask_token', 'flava.image_model.layernorm.bias', 'mim_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.input.weight', 'mmm_image_head.bias', 'flava.multimodal_model.encoder.layer.1.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.bias', 'flava.multimodal_model.cls_token', 'flava.image_model.encoder.layer.5.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.5.attention.attention.value.weight', 'flava.image_model.encoder.layer.2.output.dense.weight', 'flava.image_model.encoder.layer.7.attention.attention.key.weight', 'flava.image_model.encoder.layer.1.layernorm_before.bias', 'mmm_text_head.transform.dense.weight', 'flava.image_model.encoder.layer.6.attention.attention.query.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'flava.image_model.encoder.layer.5.attention.attention.query.weight', 'flava.image_model.encoder.layer.11.attention.attention.value.weight', 'flava.image_model.encoder.layer.10.attention.attention.value.bias', 'flava.image_model.encoder.layer.8.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.0.output.dense.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_after.bias', 'flava.image_model.encoder.layer.9.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_after.bias', 'flava.image_to_mm_projection.bias', 'mmm_image_head.decoder.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.weight', 'mmm_text_head.transform.dense.bias', 'flava.image_model.encoder.layer.6.attention.attention.value.weight', 'flava.image_model.encoder.layer.5.output.dense.bias', 'flava.image_model.encoder.layer.2.attention.attention.value.bias', 'mmm_image_head.transform.dense.bias', 'flava.image_model.encoder.layer.5.attention.output.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.11.attention.attention.query.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'flava.image_model.encoder.layer.2.attention.attention.value.weight', 'flava.image_model.encoder.layer.3.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.weight', 'flava.image_model.encoder.layer.2.layernorm_before.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.7.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.bias', 'flava.image_model.encoder.layer.7.layernorm_before.weight', 'flava.image_to_mm_projection.weight', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.bias', 'flava.image_model.encoder.layer.6.intermediate.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.11.attention.output.dense.bias', 'mim_head.decoder.bias', 'flava.image_model.encoder.layer.5.output.dense.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.bias', 'flava.multimodal_model.layernorm.weight', 'flava.image_model.encoder.layer.5.layernorm_after.bias', 'flava.image_model.encoder.layer.6.output.dense.weight', 'flava.image_model.encoder.layer.4.layernorm_before.bias', 'flava.image_model.encoder.layer.11.attention.attention.key.bias', 'flava.image_model.encoder.layer.1.intermediate.dense.weight', 'flava.image_model.encoder.layer.9.attention.output.dense.bias', 'flava.image_model.encoder.layer.5.layernorm_before.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.bias', 'flava.image_model.encoder.layer.7.intermediate.dense.bias', 'flava.image_model.encoder.layer.7.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.bias', 'flava.image_model.encoder.layer.10.attention.attention.key.bias', 'flava.image_model.encoder.layer.8.layernorm_before.weight', 'flava.image_model.encoder.layer.8.attention.attention.query.bias', 'flava.image_model.encoder.layer.3.output.dense.weight', 'flava.image_model.encoder.layer.0.output.dense.bias', 'flava.image_model.encoder.layer.0.attention.output.dense.bias', 'flava.image_model.encoder.layer.2.attention.attention.key.bias', 'flava.image_model.encoder.layer.3.output.dense.bias', 'flava.image_model.encoder.layer.11.attention.attention.key.weight', 'flava.image_model.encoder.layer.11.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'flava.multimodal_model.encoder.layer.1.layernorm_before.bias', 'flava.image_model.encoder.layer.11.layernorm_before.bias', 'mmm_image_head.transform.LayerNorm.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'flava.image_model.encoder.layer.8.attention.attention.key.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.9.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.weight', 'flava.image_model.encoder.layer.9.attention.attention.value.weight', 'flava.image_model.encoder.layer.3.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_before.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_after.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.3.intermediate.dense.weight', 'flava.image_model.encoder.layer.11.attention.attention.query.weight', 'flava.image_model.encoder.layer.2.layernorm_before.bias', 'flava.image_model.encoder.layer.7.layernorm_after.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.8.attention.attention.value.weight', 'flava.image_model.encoder.layer.2.intermediate.dense.weight', 'flava.image_model.encoder.layer.3.attention.attention.query.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.0.attention.attention.query.weight', 'flava.image_model.encoder.layer.3.layernorm_before.bias', 'flava.image_model.encoder.layer.8.layernorm_after.bias', 'flava.image_model.encoder.layer.3.attention.attention.value.bias', 'flava.image_model.encoder.layer.1.layernorm_before.weight', 'flava.image_model.encoder.layer.4.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.4.layernorm_after.bias', 'flava.image_model.encoder.layer.11.attention.output.dense.weight', 'flava.image_model.encoder.layer.10.attention.output.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.weight', 'flava.image_model.encoder.layer.1.output.dense.weight', 'itm_head.pooler.dense.bias', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'mim_head.transform.dense.bias', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.3.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.4.output.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.key.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'flava.image_model.encoder.layer.1.output.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.query.bias', 'flava.image_model.encoder.layer.10.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'flava.image_model.encoder.layer.8.output.dense.bias', 'flava.multimodal_model.encoder.layer.4.output.dense.weight', 'flava.image_model.encoder.layer.4.attention.output.dense.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'flava.image_model.encoder.layer.10.intermediate.dense.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_before.weight', 'mlm_head.transform.dense.bias', 'flava.image_model.encoder.layer.4.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.4.layernorm_before.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'mlm_head.bias', 'flava.image_model.encoder.layer.1.attention.attention.query.bias', 'mim_head.transform.LayerNorm.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.weight', 'flava.image_model.encoder.layer.9.output.dense.weight', 'flava.image_model.encoder.layer.7.attention.attention.query.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'flava.image_model.encoder.layer.5.layernorm_before.weight', 'flava.image_model.encoder.layer.0.attention.attention.query.bias', 'flava.image_model.encoder.layer.9.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.2.layernorm_after.bias', 'flava.text_to_mm_projection.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'flava.image_model.encoder.layer.6.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.weight', 'flava.image_model.encoder.layer.4.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_after.weight', 'mlm_head.decoder.bias', 'flava.image_model.encoder.layer.7.attention.attention.query.bias', 'flava.image_model.pooler.dense.bias', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.bias', 'itm_head.seq_relationship.bias', 'flava.image_model.encoder.layer.4.attention.attention.key.weight', 'flava.image_model.encoder.layer.10.intermediate.dense.weight', 'flava.image_model.encoder.layer.5.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.bias', 'flava.image_model.encoder.layer.3.layernorm_before.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias']\n",
      "- This IS expected if you are initializing FlavaTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Evaluating...:   0%|          | 0/816 [00:00<?, ?batch/s]/home/zero_lag/anaconda3/envs/multi/lib/python3.8/site-packages/transformers/modeling_utils.py:702: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Evaluating...: 100%|█████████▉| 815/816 [05:11<00:00,  2.62batch/s]\n",
      "/home/zero_lag/anaconda3/envs/multi/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from model import FlavaForNERwithESD_bert_only\n",
    "model=FlavaForNERwithESD_bert_only()\n",
    "import torch\n",
    "model.load_state_dict(torch.load('/home/zero_lag/Document/srtp/Multimodality-Link/Mert/MNER/checkpoint/epoch_1_macrof1_0.000_microf1_0.000_1659256980.bin',map_location=torch.device('cuda')))\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from utils import TwitterDataset,TwitterColloteFn,evaluate\n",
    "from config import config\n",
    "train_dataset = TwitterDataset(config.train_text_path,\n",
    "                                   config.train_img_path)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=config.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  collate_fn=TwitterColloteFn)\n",
    "W_e2n = train_dataset.W_e2n.to(config.device)\n",
    "model=model.to(config.device)\n",
    "report,pred_labels,true_labels=evaluate(model,train_dataloader,W_e2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = TwitterDataset(config.dev_text_path, config.dev_img_path)\n",
    "dev_dataloader = DataLoader(dev_dataset,\n",
    "                                batch_size=config.batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=TwitterColloteFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_e2n = W_e2n.repeat(config.batch_size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['O', 'O', 'O', 'O', 'O', 'O']\n",
      "6 ['O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "i=55\n",
    "print(len(pred_labels[i]),pred_labels[i])\n",
    "print(len(true_labels[i]),true_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('multi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b31efba31e920ff4356e1b1398f6633ecc60afc998bd8e4b4a8c6f326083d03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
