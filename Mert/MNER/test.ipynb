{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('data/twitter2015/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='exercise\tO\\n'.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding=tokenizer('I love w/ !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 1567, 192, 120, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.index('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoding.tokens())\n",
    "print(encoding.word_ids())\n",
    "print(encoding.token_to_chars(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag={0:'O',1:'B-PER',2:'I-PER',3:'B-LOC',4:'I-LOC',5:'B-ORG',6:'I-ORG',7:'B-OTHER',8:'I-OTHER'}\n",
    "tag2id={'O':0,'B-PER':1,'I-PER':2,'B-LOC':3,'I-LOC':4,'B-ORG':5,'I-ORG':6,'B-OTHER':7,'I-OTHER':8}\n",
    "ESD_id2tag={0:'O',1:'B',2:'I'}\n",
    "ESD_tag2id={'O':0,'B':1,'I':2}\n",
    "Data={}\n",
    "idx=0\n",
    "ind=0\n",
    "end=len(dataset)\n",
    "while ind < end:\n",
    "    text=dataset[ind]\n",
    "    if text[:5]=='IMGID':\n",
    "        img_id=text[6:-1]\n",
    "        ind+=1\n",
    "        if dataset[ind][:2]=='RT':# skip name\n",
    "            ind+=3\n",
    "        #read sentence\n",
    "        sentence=''\n",
    "        tags=[]\n",
    "        ESD_tags=[]\n",
    "        while ind < end:\n",
    "            text = dataset[ind]\n",
    "            if text=='\\n':#reach the end of a sample\n",
    "                ind+=1\n",
    "                break\n",
    "            if text[:4]=='http':#skip url\n",
    "                ind+=1\n",
    "                continue\n",
    "            word,tag=text.replace('\\n','').split('\\t')\n",
    "            if sentence!='':#use space to split words\n",
    "                sentence+=' '\n",
    "                tags.append(0)\n",
    "                ESD_tags.append(0)\n",
    "            sentence+=word\n",
    "            #tag is mapped to char\n",
    "            tags+=[tag2id[tag]]*len(word)\n",
    "            ESD_tags+=[ESD_tag2id[tag[0]]]*len(word)\n",
    "            ind+=1\n",
    "        Data[idx]={'imgid':img_id,'sentence':sentence,'tags':tags,'ESD_tags':ESD_tags}\n",
    "        idx+=1\n",
    "    else:\n",
    "        ind+=1\n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=b.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TwitterDataset\n",
    "from utils import TwitterColloteFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "train_data=TwitterDataset(config.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_data,batch_size=4,shuffle=True,collate_fn=TwitterColloteFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs,img_inputs,tags,ESD_tags=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.W_e2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'mim_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'mlm_head.transform.dense.bias', 'mmm_text_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.input.weight', 'mlm_head.transform.dense.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'mmm_text_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'mlm_head.bias', 'mmm_text_head.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.output.conv.weight', 'itm_head.seq_relationship.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'mim_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'mmm_image_head.decoder.bias', 'mmm_image_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'mim_head.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'mmm_text_head.decoder.weight', 'mmm_image_head.bias', 'mmm_image_head.transform.dense.weight', 'itm_head.seq_relationship.bias', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'mim_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'mlm_head.transform.LayerNorm.weight', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight']\n",
      "- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import FlavaProcessor, FlavaModel\n",
    "\n",
    "model = FlavaModel.from_pretrained(\"facebook/flava-full\")\n",
    "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open('/home/zero_lag/Document/srtp/data/twitter2015/images/27.jpg')\n",
    "\n",
    "inputs = processor(\n",
    "  text=[\"a photo of a cat\", \"a photo of a dog\"], images=[image, image], return_tensors=\"pt\", padding=\"max_length\", max_length=77\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=processor(\n",
    "  text=[\"a photo of a cat\", \"a photo of a dog and cat and cat\"], images=[image, image], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1037, 6302, 1997, 1037, 4937,  102,    0,    0,    0,    0],\n",
       "        [ 101, 1037, 6302, 1997, 1037, 3899, 1998, 4937, 1998, 4937,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[ 0.2807,  0.3829,  0.4267,  ..., -0.2886, -0.2740, -0.2886],\n",
       "          [ 0.3245,  0.3829,  0.4121,  ..., -0.2886, -0.2886, -0.3178],\n",
       "          [ 0.2807,  0.3537,  0.3683,  ..., -0.3762, -0.3470, -0.3178],\n",
       "          ...,\n",
       "          [ 1.6384,  1.5362,  1.4194,  ...,  1.3902,  1.2880,  1.2442],\n",
       "          [ 1.6092,  1.5508,  1.5070,  ...,  1.2150,  0.9814,  0.8501],\n",
       "          [ 1.6092,  1.4778,  1.4924,  ...,  0.1201, -0.1280, -0.3908]],\n",
       "\n",
       "         [[-1.3919, -1.3919, -1.3919,  ..., -1.5420, -1.5420, -1.5570],\n",
       "          [-1.3469, -1.3469, -1.3469,  ..., -1.5270, -1.5120, -1.5270],\n",
       "          [-1.4069, -1.3769, -1.3469,  ..., -1.5570, -1.5420, -1.5420],\n",
       "          ...,\n",
       "          [-0.3414, -0.4614, -0.5515,  ..., -0.6415, -0.7016, -0.7466],\n",
       "          [-0.3414, -0.3864, -0.4914,  ..., -0.7316, -0.8666, -0.9267],\n",
       "          [-0.3714, -0.4914, -0.5065,  ..., -1.2869, -1.3769, -1.4820]],\n",
       "\n",
       "         [[-0.6555, -0.4990, -0.5417,  ..., -1.0110, -0.9256, -0.9541],\n",
       "          [-0.6981, -0.5986, -0.5701,  ..., -1.0110, -0.9541, -1.0110],\n",
       "          [-0.6128, -0.5275, -0.4990,  ..., -1.0252, -1.0394, -1.0536],\n",
       "          ...,\n",
       "          [ 1.3638,  1.3496,  1.1221,  ...,  1.1647,  1.0652,  0.9514],\n",
       "          [ 1.3354,  1.1789,  1.2643,  ...,  0.9372,  0.7523,  0.6244],\n",
       "          [ 1.3780,  1.3780,  1.2643,  ..., -0.1293, -0.5559, -0.7408]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2807,  0.3829,  0.4267,  ..., -0.2886, -0.2740, -0.2886],\n",
       "          [ 0.3245,  0.3829,  0.4121,  ..., -0.2886, -0.2886, -0.3178],\n",
       "          [ 0.2807,  0.3537,  0.3683,  ..., -0.3762, -0.3470, -0.3178],\n",
       "          ...,\n",
       "          [ 1.6384,  1.5362,  1.4194,  ...,  1.3902,  1.2880,  1.2442],\n",
       "          [ 1.6092,  1.5508,  1.5070,  ...,  1.2150,  0.9814,  0.8501],\n",
       "          [ 1.6092,  1.4778,  1.4924,  ...,  0.1201, -0.1280, -0.3908]],\n",
       "\n",
       "         [[-1.3919, -1.3919, -1.3919,  ..., -1.5420, -1.5420, -1.5570],\n",
       "          [-1.3469, -1.3469, -1.3469,  ..., -1.5270, -1.5120, -1.5270],\n",
       "          [-1.4069, -1.3769, -1.3469,  ..., -1.5570, -1.5420, -1.5420],\n",
       "          ...,\n",
       "          [-0.3414, -0.4614, -0.5515,  ..., -0.6415, -0.7016, -0.7466],\n",
       "          [-0.3414, -0.3864, -0.4914,  ..., -0.7316, -0.8666, -0.9267],\n",
       "          [-0.3714, -0.4914, -0.5065,  ..., -1.2869, -1.3769, -1.4820]],\n",
       "\n",
       "         [[-0.6555, -0.4990, -0.5417,  ..., -1.0110, -0.9256, -0.9541],\n",
       "          [-0.6981, -0.5986, -0.5701,  ..., -1.0110, -0.9541, -1.0110],\n",
       "          [-0.6128, -0.5275, -0.4990,  ..., -1.0252, -1.0394, -1.0536],\n",
       "          ...,\n",
       "          [ 1.3638,  1.3496,  1.1221,  ...,  1.1647,  1.0652,  0.9514],\n",
       "          [ 1.3354,  1.1789,  1.2643,  ...,  0.9372,  0.7523,  0.6244],\n",
       "          [ 1.3780,  1.3780,  1.2643,  ..., -0.1293, -0.5559, -0.7408]]]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "image_embeddings = outputs.image_embeddings # Batch size X (Number of image patches + 1) x Hidden size => 2 X 197 X 768\n",
    "text_embeddings = outputs.text_embeddings # Batch size X (Text sequence length + 1) X Hidden size => 2 X 77 X 768\n",
    "multimodal_embeddings = outputs.multimodal_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(a,b):\n",
    "    print(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'a':1,'b':2,'c':3}\n",
    "a(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del args['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\", max_length=77)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=open(config.train_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IMGID:1015799\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.readlines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('multi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b31efba31e920ff4356e1b1398f6633ecc60afc998bd8e4b4a8c6f326083d03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
